{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1\n",
    "le code suivent permet de :\n",
    "- Importé les libs\n",
    "- Chargé les données\n",
    "- Netoyé les données\n",
    "- Cree le convertiseur NLP\n",
    "- cree l'imputer de données manquante\n",
    "- De definir la fonction pour les KPI\n",
    "- De faire tourné la main loop afin d'ajouté les KPI a `clothes_df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import string\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool, ColorPicker, Slider\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from minisom import MiniSom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "impacts_df = pd.read_csv('data.csv')\n",
    "clothes_df = pd.read_csv('vetements copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "impacts_df.fillna(0, inplace=True)\n",
    "clothes_df.fillna(0, inplace=True)\n",
    "\n",
    "null_col = []\n",
    "for col in clothes_df.columns:\n",
    "    to_del_col = True\n",
    "    for line in clothes_df[col]:\n",
    "        if line != 0.0:\n",
    "            to_del_col = False\n",
    "    if to_del_col:\n",
    "        print(f'{col} -> max: {clothes_df[col].max()} | min: {clothes_df[col].min()}')\n",
    "        null_col.append(col)\n",
    "\n",
    "clothes_df.drop(null_col, axis = 1, inplace = True)\n",
    "\n",
    "#print(impacts_df.info())\n",
    "#print(clothes_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP to convert materials into generic name\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def convert_raw_material(raw_material):\n",
    "    raw_material = raw_material.lower()\n",
    "    raw_material = raw_material.strip()\n",
    "\n",
    "    doc = nlp(raw_material)\n",
    "\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    best_match = None\n",
    "    max_similarity = 0\n",
    "    for material in impacts_df['Material']:\n",
    "        similarity = nlp(material.lower()).similarity(doc)\n",
    "        if similarity > max_similarity:\n",
    "            best_match = material\n",
    "            max_similarity = similarity\n",
    "\n",
    "    if max_similarity > 0.6:\n",
    "        #print(f\"-- OK -- {raw_material} have been transform to {best_match}\")\n",
    "        return best_match\n",
    "    else:\n",
    "        #print(f\"-- KO -- can't fine a better name for {raw_material}\")\n",
    "        return raw_material\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value imputer\n",
    "def impute_missing_value(df, column_name, k=5):\n",
    "    \n",
    "    missing_values = df[column_name].values.reshape(-1,1)\n",
    "    #print(f'missing -> {missing_values}')\n",
    "    \n",
    "    # KNNImputer\n",
    "    knn_imputer = KNNImputer(n_neighbors=k)\n",
    "    knn_imputed_values = knn_imputer.fit_transform(missing_values)\n",
    "    \n",
    "    # Linear Regression\n",
    "    lr_imputer = LinearRegression()\n",
    "    lr_imputed_values = missing_values.copy()\n",
    "    lr_imputed_values[np.isnan(lr_imputed_values)] = 0 # set NaN values to 0\n",
    "    non_missing_indices = np.where(~np.isnan(lr_imputed_values))[0] # get non-missing value indices\n",
    "    X = non_missing_indices.reshape(-1,1)\n",
    "    y = lr_imputed_values[non_missing_indices]\n",
    "    lr_imputer.fit(X, y)\n",
    "    lr_imputed_values[np.isnan(missing_values).ravel()] = lr_imputer.predict(np.isnan(missing_values).reshape(-1,1))\n",
    "    \n",
    "    # Mean imputation\n",
    "    mean_imputed_values = missing_values.copy()\n",
    "    mean = np.nanmean(mean_imputed_values)\n",
    "    mean_imputed_values[np.isnan(mean_imputed_values)] = mean\n",
    "    \n",
    "    # Evaluate performance\n",
    "    knn_mae = np.mean(abs(missing_values - knn_imputed_values))\n",
    "    lr_mae = np.mean(abs(missing_values - lr_imputed_values))\n",
    "    mean_mae = np.mean(abs(missing_values - mean_imputed_values))\n",
    "    \n",
    "    # Return imputed value with best performance\n",
    "    if knn_mae <= lr_mae and knn_mae <= mean_mae:\n",
    "        return knn_imputed_values[0][0]\n",
    "    elif lr_mae <= mean_mae:\n",
    "        return lr_imputed_values[0][0]\n",
    "    else:\n",
    "        return mean_imputed_values[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0 KPI calculatator\n",
    "def calculate_kpis(row):\n",
    "\n",
    "    impact_data = impacts_df.loc[impacts_df['Material'] == row]\n",
    "    \n",
    "    if impact_data.isnull().values.any():\n",
    "        for col in impact_data.columns:\n",
    "            if impact_data[col].isnull().values.any():\n",
    "                imputed_value = impute_missing_value(impact_data, col)\n",
    "                impact_data[col] = imputed_value\n",
    "    \n",
    "    \n",
    "    env_score = (impact_data['Water_Use_kg/kg'] + impact_data['Fossil_Energy_kg/kg'] +\n",
    "                 impact_data['Greenhouse_Gas_kgCO2eq/kg'] + impact_data['Land_use_m2/kg']) / 4\n",
    "    \n",
    "    animal_score = impact_data['Animal_Welfare_Score']\n",
    "    \n",
    "    \n",
    "    human_score = impact_data['Human_Welfare_Score']\n",
    "    \n",
    "    \n",
    "    social_score = (1 if impact_data['Labor_Conditions'].eq(\"unsafe\").any() else 10+ \n",
    "                    impact_data['Human_Welfare_Score']) / 2\n",
    "\n",
    "    kpis = {'Environmental Impact Score': float(env_score),\n",
    "            'Animal Welfare Score': float(animal_score),\n",
    "            'Human Welfare Score': float(human_score),\n",
    "            'Social Responsibility Score': float(social_score)}\n",
    "    \n",
    "    return kpis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 KPI calculator\n",
    "def normalize_value(value, min_value, max_value, new_min=0, new_max=10):\n",
    "    out = new_min + ((value - min_value) * (new_max - new_min)) / (max_value - min_value)\n",
    "    return out\n",
    "\n",
    "def calculate_kpis(row):\n",
    "\n",
    "    impact_data = impacts_df.loc[impacts_df['Material'] == row]\n",
    "    \n",
    "    if impact_data.isnull().values.any():\n",
    "        for col in impact_data.columns:\n",
    "            if impact_data[col].isnull().values.any():\n",
    "                imputed_value = impute_missing_value(impact_data, col)\n",
    "                impact_data[col] = imputed_value\n",
    "    \n",
    "    env_score = (impact_data['Water_Use_kg/kg'] + impact_data['Fossil_Energy_kg/kg'] +\n",
    "                 impact_data['Greenhouse_Gas_kgCO2eq/kg'] + impact_data['Land_use_m2/kg']) / 4\n",
    "    \n",
    "    animal_score = impact_data['Animal_Welfare_Score']\n",
    "    human_score = impact_data['Human_Welfare_Score']\n",
    "    social_score = (1 if impact_data['Labor_Conditions'].eq(\"unsafe\").any() else 10 + \n",
    "                    impact_data['Human_Welfare_Score']) / 2\n",
    "\n",
    "    # Define the minimum and maximum values for each KPI\n",
    "    env_min, env_max = 0, 58\n",
    "    animal_min, animal_max = 0, 5\n",
    "    human_min, human_max = 0, 5\n",
    "    social_min, social_max = 0, 5\n",
    "\n",
    "    # Normalize the KPI values to a range of 0-10\n",
    "    env_score_normalized = normalize_value(float(env_score), env_min, env_max)\n",
    "    animal_score_normalized = normalize_value(float(animal_score), animal_min, animal_max)\n",
    "    human_score_normalized = normalize_value(float(human_score), human_min, human_max)\n",
    "    social_score_normalized = normalize_value(float(social_score), social_min, social_max)\n",
    "\n",
    "    kpis = {'Environmental Impact Score': env_score_normalized,\n",
    "            'Animal Welfare Score': animal_score_normalized,\n",
    "            'Human Welfare Score': human_score_normalized,\n",
    "            'Social Responsibility Score': social_score_normalized}\n",
    "    \n",
    "    return kpis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main KPI Loops\n",
    "times = []\n",
    "for idx, raw in clothes_df.iterrows():\n",
    "    kpis_row = []\n",
    "    final = {}\n",
    "    start_time = perf_counter()\n",
    "    for col, value in raw.items():\n",
    "        if (value not in [0, 0.0]) and (col != \"Type\"):\n",
    "            kpis = calculate_kpis(convert_raw_material(col))\n",
    "            pond_kpis = {}\n",
    "            for k, v in kpis.items():\n",
    "                pond_kpis[k] = v * value\n",
    "            kpis_row.append(pond_kpis)\n",
    "    \n",
    "    for i in kpis_row:\n",
    "        for k, v in i.items():\n",
    "            final[k] = final.get(k, 0) + (v/len(kpis_row))\n",
    "    for k, v in final.items():\n",
    "        if k not in clothes_df.columns:\n",
    "            clothes_df[k] = 0\n",
    "        clothes_df.loc[idx, k] = round(v, 2)\n",
    "    if idx % 10 == 0:\n",
    "        print(f'Index {idx} done. Total {len(clothes_df)}. Percentage -> {(idx / len(clothes_df)):.2%}')\n",
    "    times.append(perf_counter() - start_time)\n",
    "\n",
    "print(clothes_df)\n",
    "print(f'mean time per row -> {(sum(times)/len(times)):.2f} s')\n",
    "print(f'max time per row -> {max(times):.2f} s')\n",
    "print(f'min time per row -> {min(times):.2f} s')\n",
    "print(f'total time -> {sum(times):.2f} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2\n",
    "Le code suivant permet de visualisé le dataset dans ca globalité grace a: \n",
    "- Un plot 2D affichant a densité des données avec une reduction de dimentionalité (t-SNE)\n",
    "- 4 polot 3D affichant des cluster colorié en fonction des KPI avec une reduction de dimentionalité (t-SNE)\n",
    "- Un plot de coordonée parallel afin de l'ensemble des variable de mon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D plot TSNE\n",
    "df = clothes_df.drop(['Type', 'Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score'], axis=1)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "transformed_data = tsne.fit_transform(df)\n",
    "\n",
    "# Créer un DataFrame avec les données transformées\n",
    "tsne_df = pd.DataFrame(transformed_data, columns=['t-SNE 1', 't-SNE 2'])\n",
    "\n",
    "p = figure(match_aspect=True, tools=\"wheel_zoom,reset\")\n",
    "p.background_fill_color = '#440154'\n",
    "p.grid.visible = False\n",
    "\n",
    "p.hexbin(tsne_df['t-SNE 1'], tsne_df['t-SNE 2'], size=10, hover_color=\"pink\", hover_alpha=0.8)\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"count\", \"@c\"), (\"(q,r)\", \"(@q, @r)\")])\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plot TSNE\n",
    "df = clothes_df.drop(['Type', 'Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score'], axis=1)\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "transformed_data = tsne.fit_transform(df)\n",
    "\n",
    "# Créer un DataFrame avec les données transformées\n",
    "tsne_df = pd.DataFrame(transformed_data, columns=['t-SNE 1', 't-SNE 2', 't-SNE 3'])\n",
    "\n",
    "# Ajouter les colonnes de scores au DataFrame tsne_df\n",
    "score_columns = ['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score']\n",
    "tsne_df[score_columns] = clothes_df[score_columns]\n",
    "\n",
    "# Créer et afficher des graphiques séparés pour chaque KPI\n",
    "for score in score_columns:\n",
    "    fig = px.scatter_3d(tsne_df, x='t-SNE 1', y='t-SNE 2', z='t-SNE 3',\n",
    "                        color=score, color_continuous_scale='Viridis', opacity=0.7,\n",
    "                        title=f\"Visualisation du dataset avec t-SNE (3D) - {score}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Coordinates\n",
    "# ... (chargement et normalisation des données)\n",
    "plotly.io.orca.config.executable = '/path/to/orca'\n",
    "def create_parallel_coordinates_plot(color_column):\n",
    "    # Créez une palette de couleurs prédéfinie (du violet au orange)\n",
    "    colorscale = px.colors.sequential.Viridis\n",
    "    \n",
    "    # Normalisez la colonne sélectionnée pour les couleurs\n",
    "    color_values = (normalized_df[color_column] - normalized_df[color_column].min()) / \\\n",
    "                   (normalized_df[color_column].max() - normalized_df[color_column].min())\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    for index, row in normalized_df.iterrows():\n",
    "        color_index = int(color_values.loc[index] * (len(colorscale) - 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=target_columns,\n",
    "                y=row,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=colorscale[color_index], width=0.5),\n",
    "                opacity=0.1,\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Personnaliser les axes\n",
    "    fig.update_xaxes(\n",
    "        tickangle=90,  # Régler l'angle des étiquettes des axes à 90 degrés (vertical)\n",
    "        tickfont=dict(size=10),  # Changer la taille de la police des étiquettes des axes\n",
    "        title_font=dict(size=12),  # Changer la taille de la police des titres des axes\n",
    "        tickcolor=\"gray\",  # Changer la couleur des graduations des axes\n",
    "        showgrid=True,  # Afficher la grille\n",
    "        gridcolor=\"lightgray\",  # Changer la couleur de la grille\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        tickfont=dict(size=10),  # Changer la taille de la police des étiquettes des axes\n",
    "        title_font=dict(size=12),  # Changer la taille de la police des titres des axes\n",
    "        tickcolor=\"gray\",  # Changer la couleur des graduations des axes\n",
    "        showgrid=True,  # Afficher la grille\n",
    "        gridcolor=\"lightgray\",  # Changer la couleur de la grille\n",
    "        tickvals=[]  # Supprimer les échelles des axes\n",
    "    )\n",
    "\n",
    "    # Personnaliser l'apparence du graphique\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"Diagramme parallèle (Parallel Coordinates) du dataset\",\n",
    "            font=dict(size=18)\n",
    "        ),\n",
    "        plot_bgcolor=\"white\",  # Changer la couleur de fond du graphique\n",
    "        width=800,\n",
    "        height=500,\n",
    "        margin=dict(l=100, r=100, t=100, b=100)  # Ajuster l'espacement entre les axes et les bords du graphique\n",
    "    )\n",
    "\n",
    "    fig.write_image('test.svg', engine='orca')\n",
    "    fig.show()\n",
    "\n",
    "# Créez un widget Dropdown pour sélectionner la colonne de couleur\n",
    "interact(create_parallel_coordinates_plot, color_column=target_columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "\n",
    "features = clothes_df.iloc[:, -4:]\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "['KMeanCluster', 'DbscanCluster', 'GMCluster', 'AggCluster', 'SpectralCluster', 'SpectralCluster', 'OpticsCluster', 'MeanShiftCluster', 'AffinityPropCluster', 'BirchCluster', 'SOMCluster', 'HDBSCANCluster']\n",
    "#Elboy pour trouvé le best nombre de class\n",
    "#K medoid \n",
    "#GMM Model pour annalysé le dataset -> graph 2,5D\n",
    "#Kmean \n",
    "\n",
    "#KMean\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(features)\n",
    "predictions = kmeans.predict(features)\n",
    "clothes_df[\"KMeanCluster\"] = predictions\n",
    "\n",
    "#DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(features)\n",
    "predictions = dbscan.labels_\n",
    "clothes_df[\"DbscanCluster\"] = predictions\n",
    "\n",
    "#GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(features)\n",
    "predictions = gmm.predict(features)\n",
    "clothes_df[\"GMCluster\"] = predictions\n",
    "\n",
    "# Agglomerative Hierarchical Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\n",
    "predictions = agg_clustering.fit_predict(features)\n",
    "clothes_df[\"AggCluster\"] = predictions\n",
    "\n",
    "# Spectral Clustering\n",
    "spectral_clustering = SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\")\n",
    "predictions = spectral_clustering.fit_predict(features)\n",
    "clothes_df[\"SpectralCluster\"] = predictions\n",
    "\n",
    "# OPTICS\n",
    "optics = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)\n",
    "predictions = optics.fit_predict(features)\n",
    "clothes_df[\"OpticsCluster\"] = predictions\n",
    "\n",
    "# Mean Shift Clustering\n",
    "mean_shift = MeanShift(bandwidth=None, bin_seeding=True)\n",
    "predictions = mean_shift.fit_predict(features)\n",
    "clothes_df[\"MeanShiftCluster\"] = predictions\n",
    "\n",
    "# Affinity Propagation\n",
    "affinity_propagation = AffinityPropagation(random_state=42)\n",
    "predictions = affinity_propagation.fit_predict(features)\n",
    "clothes_df[\"AffinityPropCluster\"] = predictions\n",
    "\n",
    "# Birch\n",
    "birch = Birch(n_clusters=2, threshold=0.5, branching_factor=50)\n",
    "predictions = birch.fit_predict(features)\n",
    "clothes_df[\"BirchCluster\"] = predictions\n",
    "\n",
    "# Self-Organizing Map\n",
    "som_grid_rows, som_grid_columns = 2, 2\n",
    "som = MiniSom(som_grid_rows, som_grid_columns, features.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)\n",
    "som.train_random(features, num_iteration=1000)\n",
    "predictions = [som.winner(x) for x in features]\n",
    "clothes_df[\"SOMCluster\"] = predictions\n",
    "\n",
    "# HDBSCAN\n",
    "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "predictions = hdbscan_cluster.fit_predict(features)\n",
    "clothes_df[\"HDBSCANCluster\"] = predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audi cluster\n",
    "def ultimate_clustered(cluster):\n",
    "    print(f'{cluster} en cours')\n",
    "    \n",
    "    print(f'generation des plots')\n",
    "    sns.set(style=\"ticks\", color_codes=True)\n",
    "    sns.pairplot(data=clothes_df, vars=['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score'], hue=cluster, diag_kind='hist')\n",
    "    plt.savefig(f\"images/{cluster.replace('Cluster','')}_pairplot.png\", dpi=300)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for i in range(len(clothes_df[cluster].unique())):\n",
    "        cluster_df = clothes_df[clothes_df[cluster] == i]\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=cluster_df.mean()[['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score']].tolist(),\n",
    "            theta=['Environmental Impact', 'Animal Welfare', 'Human Welfare', 'Social Responsibility'],\n",
    "            fill='toself',\n",
    "            name=f'Cluster {i}'\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 10]\n",
    "        )),\n",
    "        showlegend=True\n",
    "    )\n",
    "    print(f\"images/{cluster.replace('Cluster','')}_radar.png\")\n",
    "    fig.write_image(f\"images/{cluster.replace('Cluster','')}_radar.png\")\n",
    "    fig.show()\n",
    "    \n",
    "    kmeans_means = clothes_df.groupby(cluster).mean()[['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score']]\n",
    "    kmeans_means.plot(kind='bar')\n",
    "    plt.title(f'Mean KPI Scores by {cluster}')\n",
    "    plt.savefig(f\"images/{cluster.replace('Cluster','')}_histo.png\", dpi=300)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "clusters = ['KMeanCluster', 'DbscanCluster', 'GMCluster', 'AggCluster', 'SpectralCluster', 'SpectralCluster', 'OpticsCluster', 'MeanShiftCluster', 'AffinityPropCluster', 'BirchCluster', 'SOMCluster', 'HDBSCANCluster']\n",
    "cluster = clusters[3]\n",
    "ultimate_clustered(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intern KPI of cluster\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Génération de données pour l'exemple\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=42)\n",
    "\n",
    "# Exemple de clustering avec KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Calcul du coefficient de silhouette\n",
    "silhouette_avg = silhouette_score(X, y_pred)\n",
    "silhouette_indicator = \"bon\" if silhouette_avg > 0.5 else \"mauvais\"\n",
    "print(f\"Coefficient de silhouette : {silhouette_avg} ({silhouette_indicator})\")\n",
    "\n",
    "# Calcul de l'indice de Davies-Bouldin\n",
    "dbi = davies_bouldin_score(X, y_pred)\n",
    "dbi_indicator = \"bon\" if dbi < 0.5 else \"mauvais\"\n",
    "print(f\"Indice de Davies-Bouldin : {dbi} ({dbi_indicator})\")\n",
    "\n",
    "# Calcul de l'indice de Calinski-Harabasz\n",
    "ch_score = calinski_harabasz_score(X, y_pred)\n",
    "ch_indicator = \"bon\" if ch_score > (len(X) / 2) else \"mauvais\"\n",
    "print(f\"Indice de Calinski-Harabasz : {ch_score} ({ch_indicator})\")\n",
    "\n",
    "#Nethode elbow\n",
    "#kullerback \n",
    "# Attribution d'un score global au modèle en se basant sur les 3 méthodes\n",
    "total_score = (\n",
    "    (silhouette_avg - 0.5) / 0.5 +\n",
    "    (1 - dbi) +\n",
    "    (ch_score - len(X) / 2) / (len(X) / 2)\n",
    ")\n",
    "total_score /= 3\n",
    "print(f\"Score global du modèle : {total_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non null kpi data\n",
    "nonzero_rows = clothes_df[\n",
    "    (clothes_df['Environmental Impact Score'] != 0.00) |\n",
    "    (clothes_df['Animal Welfare Score'] != 0.00) |\n",
    "    (clothes_df['Human Welfare Score'] != 0.00) |\n",
    "    (clothes_df['Social Responsibility Score'] != 0.00)\n",
    "]\n",
    "\n",
    "# Print the selected rows, along with the other columns\n",
    "print(nonzero_rows[['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score', cluster]].head(600).to_csv(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PairPlot\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(data=clothes_df, vars=['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score'], hue=cluster, diag_kind='hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radar\n",
    "\n",
    "fig = go.Figure()\n",
    "def radar(cluster):\n",
    "  for i in range(len(clothes_df[cluster].unique())):\n",
    "      cluster_df = clothes_df[clothes_df[cluster] == i]\n",
    "      fig.add_trace(go.Scatterpolar(\n",
    "          r=cluster_df.mean()[['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score']].tolist(),\n",
    "          theta=['Environmental Impact', 'Animal Welfare', 'Human Welfare', 'Social Responsibility'],\n",
    "          fill='toself',\n",
    "          name=f'Cluster {i}'\n",
    "      ))\n",
    "\n",
    "  fig.update_layout(\n",
    "    polar=dict(\n",
    "      radialaxis=dict(\n",
    "        visible=True,\n",
    "        range=[0, 10]\n",
    "      )),\n",
    "    showlegend=True\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "radar(cluster)\n",
    "#radar('DbscanCluster')\n",
    "#radar('GMCluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bars\n",
    "\n",
    "# Bar plot of mean KPI scores by cluster\n",
    "kmeans_means = clothes_df.groupby(cluster).mean()[['Environmental Impact Score', 'Animal Welfare Score', 'Human Welfare Score', 'Social Responsibility Score']]\n",
    "kmeans_means.plot(kind='bar')\n",
    "plt.title(f'Mean KPI Scores by {cluster}')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetIA",
   "language": "python",
   "name": "projetia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ce18b8474af4ae67bf63c2fa550d982c546a18a7e43f78321c82005b79bfd87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
